According to Vice President of Engineering at Google, Amin Vahdat, we are at a once-in-a-generation inflection point in computing. It means that there are new requirements for new generations. One of the new requirements is sustainability. EPerf is an open-source tool that is designed to infer the energy consumption of an application at a granular level. It uses micro-architectural events like cache and TLB misses to predict energy consumption and builds linear models to infer the power usage associated with each hardware event. It predicts energy consumption with an error rate of 1.2% to 14% (mean error of 6%) on a single socket server and 3.7% to 31.6% (mean error of 19%) on a double socket machine. RAPL (Running Average Power Limit) counters are used to create a model that can predict the energy consumption of a given application. RAPL measurements are taken by directly reading the MSRs using the privileged rdmsr instruction to get access to their values. These values are then multiplied by processor-specific RAPL units to convert them into joules. Certain restrictions are implemented to ensure correct energy readings, such as using taskset to pin the process being measured to a specific core, ensuring that test programs run for much longer than 1 ms, oversampling the counters and throwing away repeating values, and using the reported value of the Package domain. Linux perf measures many different micro-architectural performance counters like the number of instructions retired, number of stalls, L1 instruction and data cache misses, L2 and last level cache misses, and instruction and data TLB misses. On the other hand, CVXP, a convex optimization solver, creates a linear model which minimizes the relative error between the actual energy readings and the predicted value. SPEC 2017 and GAPbs benchmarks are used to build the model.
However, it is currently only tested at the given process level and not the subroutine level. On top of that, it currently ignores the extra energy used by cross-socket memory operations, giving a higher error for multi-socket servers, and is less accurate when the model is used to predict across processor families. RAPL does have limitations with its implementation, which impacts how the data are gathered, and Linux perf collects only a few percent of overhead at 20,000 samples per second.
I hope we can fix some issues like making API and testing them on unit tests such as error handlings or each function call to test subroutine-wise. 
Also, we can expand this project to other types of energy consumption like network energy, and invent power management techniques to reduce energy consumption with a given prediction.